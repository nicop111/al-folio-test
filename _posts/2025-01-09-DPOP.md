---
layout: post
title: DPOP
date: 2025-01-09 14:00:00 +0100
description: Summary of Dynamic Programming and Optimal Control course at ETH Zürich
categories: [Uni Courses]
tags: [Optimal Control, Dynamic Programming, Bellman Equation]
related_posts: false
toc:
  beginning: true
  sidebar: left
---


> This post is work in progress
{: .block-warning }

# Discrete-time Systems

The dynamics of a discrete-time system in standard form are given by

$$x_{k+1} = f_k(x_k, u_k, w_k), \quad k = 0,...,N-1\label{eq:discrete_system}$$

- $$x_k \in \mathcal{S}_k$$ - System state at time $$k$$, where $$\mathcal{S}_k$$ are possible states at that time
- $$u_k \in \mathcal{U}_k(x_k)$$ - Control input at time $$k$$. The set of allowable inputs $$\mathcal{U}_k(x_k)$$ can depend on the state itself.
- $$w_k$$ - Disturbance at time $$k$$ whose probability distribution $$p_{w_k \mid x_k, u_k} (\bullet \mid x_k, u_k) $$ is given. In particular it is independent of all prior $$x_l, u_l, w_l, l < k$$.
- $$N$$ - Time horizon

## Discrete-state Systems
This section introduces Systems, where the time and the state-space is discrete (also known as Markov Decision Processes).
If furthermore the state space is finite we call them finite-state Systems.
Generally in \eqref{eq:discrete_system} the states live in a continuous space. 
If they are discrete we can use an alternative way to describe the system, that is transition probabilities

$$P_{ij}(u,k) = \text{Pr}(x_{k+1} = j | x_k=i,u_k=u) = p_{x_{k+1} | x_k,u_k}(j|i,u)$$

where $$p_{x_k+1 | x_k,u_k}(\cdot|\cdot,\cdot)$$ denotes the probability of $x_{k+1}$ given $x_k$ and $u_k$.
In other words it describes the probability to get us from state $i$ to state $j$ with control input $u$ at time $k$.
This is equivalent to the system
$$x_{k+1} = w_k$$
where $w_k$ has the probability distribution
$$p_{w_k | x_k,u_k}(j|i,u)$$.

If \eqref{eq:discrete_system} is a discrete-state system, its transition probabilities are

$$P_{ij}(u,k) = \sum_{\{\bar w_k | f_k(i,u,\bar w_k) = j\}} p_{w_k | x_k,u_k}(\bar w_k |i,u)$$





## Non-standard Problems

Some problems are not given in the general form, but can be reformulated as such.

### Time lags

If the dynamics are of the form $$x_{k+1} = f_k(x_k, x_{k-1}, u_k, u_{k-1}, w_k)$$ we can construct a state vector $$\tilde x_k := (x_k, y_k, s_k)$$ and modify the dynamics like

$$\tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, w_k) = \left[\begin{matrix}
f_k(x_k, y_k, u_k, s_k, w_k)\\
x_k\\ 
u_k
\end{matrix}\right]$$

yielding a system in standard form. For even longer time lags we just introduce more variables in the augmented state.

### Correlated Disturbances

Disturbances $$w_k$$ correlated over time can sometimes be modeled as

$$y_{k+1} = A_k y_k + \xi_k , \quad   w_k = C_ky_{k+1}$$

Where $$A_k$$ and $$C_k$$ are given and $$\xi_k$$ are independent random variables. 
Then we can augment the state as $$\tilde x_k = (x_k, y_k)$$ and update the dynamics:

$$ \tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, \xi_k) = \begin{bmatrix}
f_k(x_k, u_k, C_k(A_ky_k + \xi_k))\\
A_ky_k + \xi_k
\end{bmatrix} $$


### Forecasts

The disturbance's probability distribution is dependent on a forecast $$y_k$$ that is known at each timestep: $$w_k \sim p_{w_k|y_k}(\cdot | i)$$. 
The forecast itself gets determined in the prior time step by a random variable $$y_{k+1} = \xi_k$$ that has a given a priori distribution $$\xi_k \sim p_{\xi_k}(i)$$. 
It is in particular independent of all states, inputs and disturbances.

We augmented the state vector $$\tilde x_k = (x_k, y_k)$$ and the disturbance 
$$\tilde w_k = (w_k, \xi_k)$$ with distribution $$p(\tilde w_k|\tilde x_k, u_k) = p(w_k|y_k)p(\xi_k)$$.
This gives us the dynamics in standard form:

$$\tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, \tilde w_k) = \begin{bmatrix}
f_k(x_k, u_k, w_k)\\
\xi_k
\end{bmatrix}$$

<!-- The DPA then becomes:

$$\begin{align*}
J_N(\tilde{\text x}) &= J_N(\text x, \text y) = g_N(\text x), \quad \text x\in\mathcal{S}_N, \text y \in \{1, ..., m\}\\
J_k(\tilde{\text x}) &= J_k(\text x, \text y) = \min_{\text u\in\mathcal{U}_k(x_k)} \E_{(w_k | y_k = \text y)} \\
&\left[ g_k(\text x, \text u, w_k) + \sum_{i=1}^m p_{\xi_k}(i) J_{k+1}(f_k(\text x, \text u, w_k), i) \right] \\
&\forall \text x \in \mathcal S_k, \, \forall \text y \in \{1, ..., m\}, \, \forall k = N-1, ..., 0.
\end{align*}$$ -->

---
# Finite Horizon Problems

Given an initial state $$x_0$$ our goal is to find an optimal control input to the system $$\ref{eq:discrete_system}$$, that minimizes the cost function

$$
\underbrace{g_N(x_N)}_{\text{terminal cost}} + \underbrace{\sum_{k=0}^{N-1}\underbrace{g_k(x_k,u_k,w_k)}_\text{stage cost}}_\text{accumulated cost}
\label{eq:cost_function}$$

while the horizon $$N$$ is finite. 
Notice that $$W_0 = (w_0,...,w_{N-1})$$ are random variables and thereby $$X_1 = (x_1,...,x_{N})$$ are generally random variables as well, because they are correlated through the system dynamics.
We therefore define the expected cost as

$$\text{E}_{(X_1,W_0|x_0)} \left[  g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k,u_k,w_k)  \right]
\label{eq:exp_cost}$$

## Open-loop vs. Closed-loop control
**Open-loop control** means, there is no feedback and the entire control sequence is determined in the beginning.
Given an initial state $$x_0$$, we search a fixed sequence of control inputs $$U = (u_0,...,u_{N-1})$$ that minimizes
$$\ref{eq:exp_cost}$$. 

In **Closed-loop control** at each timestep we know the current state.
A policy $$\pi = (\mu_0(\cdot), ..., \mu_{N-1}(\cdot))$$ maps each state at every timestep to a control input $$u_k = \mu_k(x_k)\in\mathcal{U}_k(x_k)$$. 
Thus there is a feedback from the current state to the control input. 
Given an initial state $$x_0$$, the expected cost associated with $$\pi$$ becomes

$$
J^\pi(x_0) = \text{E}_{(X_1,W_0|x_0)} \left[  g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k,\mu_k(x_k),w_k)  \right]
\label{eq:policy_exp_cost}$$

We look for an optimal policy $$\pi^*$$ which minimizes the expected cost, yielding the optimal cost
$$J^*(x) := J^{\pi^*}(x) \leq J^\pi(x), \forall \pi, \forall x\in\mathcal{S}_0.$$

Without disturbance the system, and thus the cost, becomes deterministic. 
There is no benefit in a feedback, i.e. Open-loop and Closed-loop control are equivalent.

## Principle of Optimality
The Principle of Optimality makes it possible to split up the optimization problem into smaller subproblems.
Casually speaking, when you have an optimal policy from time $$0$$ to the end, then the policy is also optimal from an arbitrary time $$i$$ to the end.

>Let $$\pi^* = (\mu_0^*(\cdot), ..., \mu_i^*(\cdot), ..., \mu_{N-1}^*(\cdot))$$ be an optimal policy. 
>Consider the subproblem whereby we are at $$x_i \in S_i$$ at time $$i$$ with $$X_{i+1} = (x_{i+1},...,x_{N})$$ and $$W_{i} = (x_i,...,x_{N})$$. 
>We want to minimize the cost from this time-step up until the end of the original problem
>
>$$J_i(x_i) = \text{E}_{(X_{i+1},W_i|x_i)} \left[  g_N(x_N) + \sum_{k=i}^{N-1}g_k(x_k,\mu_k(x_k),w_k)  \right]$$
> 
>Then the truncated policy $$(\mu_i^*(\cdot), ..., \mu_{N-1}^*(\cdot))$$ is optimal for the subproblem and for all possible $$x_i$$.
>If it wasn't, there would exist a policy yielding a lower cost on at least some portion of the state space.
>But this contradicts that $$\pi^*$$ was an optimal policy in the first place.
{: .notice--info}

## The Dynamic Programming Algorithm
Dynamic Programming uses the Principle of Optimality to recursively compute an optimal closed-loop policy and an optimal cost.
Beginning from the terminal states at time N, going backwards towards time step 0.

>Initialization
>
>$$J_N(x) = g_N(x), \quad \forall x \in \mathcal{S}_N$$
>
>Recursion $$k=(N-1),...,0$$: For each $$x_k \in \mathcal{S}_k$$ compute
>
>$$
>J_k(x_k) = \min_{u\in\mathcal{U}_k(x)} \text{E}_{(w_k | x_k=x,u=u)} \left[ g_k(x_k,u,w_k) + J_{k+1}(f_k(x_k,u,w_k)) \right] 
>$$
>
> This gives us the optimal cost function $$J^*(x_0) = J_0(x_0)$$.
> Furthermore the minimizing $$u^∗ =: µ^*_k (x_k)$$ yields the optimal policy $$π^∗ = (µ^∗_0 (·) , µ^∗_1 (·) , . . . , µ^∗_{N −1} (·))$$
{: .notice--info}

## Deterministic Finite State Systems and the Shortest Path Problem

A Deterministic Finite State System is a special case of the discrete-state system: Namely the number of states is finite $$|S_k|<\infty$$. 
Furthermore the system is deterministic i.e. it has no disturbance $$w_k = 0$$, which means we know exactly how our system will evolve given a certain input.
As shown later, these DFS systems are equivalent to the Shortest Path Problem.

---
# Infinite Horizon Problems
Bellman equation

In the following we will exlusively deal with [Finite-state Systems](#discrete-state-systems) and we want to minimize the expected cost as $N$ goes to infinity.
## Value Iteration
## Policy Iteration
## Linear Programming

---
# Deterministic Continuous Time Problems
Bellmann Equation 

$$123$$

### Pontyagin's Minimum Principle
Extra: How to derive the LQR from Pontyagins Principle
