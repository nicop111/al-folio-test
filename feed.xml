<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nicop111.github.io/al-folio-test/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nicop111.github.io/al-folio-test/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-19T13:37:07+00:00</updated><id>https://nicop111.github.io/al-folio-test/feed.xml</id><title type="html">nicopedia</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DPOP</title><link href="https://nicop111.github.io/al-folio-test/2025/DPOP/" rel="alternate" type="text/html" title="DPOP"/><published>2025-01-09T13:00:00+00:00</published><updated>2025-01-09T13:00:00+00:00</updated><id>https://nicop111.github.io/al-folio-test/2025/DPOP</id><content type="html" xml:base="https://nicop111.github.io/al-folio-test/2025/DPOP/"><![CDATA[<blockquote class="notice--danger"> <p>This post is work in progress</p> </blockquote> <h1 id="discrete-time-systems">Discrete-time Systems</h1> <p>The dynamics of a discrete-time system in standard form are given by</p> \[x_{k+1} = f_k(x_k, u_k, w_k), \quad k = 0,...,N-1\label{eq:discrete_system}\] <ul> <li>\(x_k \in \mathcal{S}_k\) - System state at time \(k\), where \(\mathcal{S}_k\) are possible states at that time</li> <li>\(u_k \in \mathcal{U}_k(x_k)\) - Control input at time \(k\). The set of allowable inputs \(\mathcal{U}_k(x_k)\) can depend on the state itself.</li> <li>\(w_k\) - Disturbance at time \(k\) whose probability distribution \(p_{w_k \mid x_k, u_k} (\bullet \mid x_k, u_k)\) is given. In particular it is independent of all prior \(x_l, u_l, w_l, l &lt; k\).</li> <li>\(N\) - Time horizon</li> </ul> <h2 id="discrete-state-systems">Discrete-state Systems</h2> <p>This section introduces Systems, where the time and the state-space is discrete (also known as Markov Decision Processes). If furthermore the state space is finite we call them finite-state Systems. Generally in \eqref{eq:discrete_system} the states live in a continuous space. If they are discrete we can use an alternative way to describe the system, that is transition probabilities</p> \[P_{ij}(u,k) = \text{Pr}(x_{k+1} = j | x_k=i,u_k=u) = p_{x_{k+1} | x_k,u_k}(j|i,u)\] <p>where \(p_{x_k+1 | x_k,u_k}(\cdot|\cdot,\cdot)\) denotes the probability of $x_{k+1}$ given $x_k$ and $u_k$. In other words it describes the probability to get us from state $i$ to state $j$ with control input $u$ at time $k$. This is equivalent to the system \(x_{k+1} = w_k\) where $w_k$ has the probability distribution \(p_{w_k | x_k,u_k}(j|i,u)\).</p> <p>If \eqref{eq:discrete_system} is a discrete-state system, its transition probabilities are</p> \[P_{ij}(u,k) = \sum_{\{\bar w_k | f_k(i,u,\bar w_k) = j\}} p_{w_k | x_k,u_k}(\bar w_k |i,u)\] <h2 id="non-standard-problems">Non-standard Problems</h2> <p>Some problems are not given in the general form, but can be reformulated as such.</p> <h3 id="time-lags">Time lags</h3> <p>If the dynamics are of the form \(x_{k+1} = f_k(x_k, x_{k-1}, u_k, u_{k-1}, w_k)\) we can construct a state vector \(\tilde x_k := (x_k, y_k, s_k)\) and modify the dynamics like</p> \[\tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, w_k) = \left[\begin{matrix} f_k(x_k, y_k, u_k, s_k, w_k)\\ x_k\\ u_k \end{matrix}\right]\] <p>yielding a system in standard form. For even longer time lags we just introduce more variables in the augmented state.</p> <h3 id="correlated-disturbances">Correlated Disturbances</h3> <p>Disturbances \(w_k\) correlated over time can sometimes be modeled as</p> \[y_{k+1} = A_k y_k + \xi_k , \quad w_k = C_ky_{k+1}\] <p>Where \(A_k\) and \(C_k\) are given and \(\xi_k\) are independent random variables. Then we can augment the state as \(\tilde x_k = (x_k, y_k)\) and update the dynamics:</p> \[\tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, \xi_k) = \begin{bmatrix} f_k(x_k, u_k, C_k(A_ky_k + \xi_k))\\ A_ky_k + \xi_k \end{bmatrix}\] <h3 id="forecasts">Forecasts</h3> <p>The disturbance’s probability distribution is dependent on a forecast \(y_k\) that is known at each timestep: \(w_k \sim p_{w_k|y_k}(\cdot | i)\). The forecast itself gets determined in the prior time step by a random variable \(y_{k+1} = \xi_k\) that has a given a priori distribution \(\xi_k \sim p_{\xi_k}(i)\). It is in particular independent of all states, inputs and disturbances.</p> <p>We augmented the state vector \(\tilde x_k = (x_k, y_k)\) and the disturbance \(\tilde w_k = (w_k, \xi_k)\) with distribution \(p(\tilde w_k|\tilde x_k, u_k) = p(w_k|y_k)p(\xi_k)\). This gives us the dynamics in standard form:</p> \[\tilde x_{k+1} = \tilde f_k(\tilde x_k, u_k, \tilde w_k) = \begin{bmatrix} f_k(x_k, u_k, w_k)\\ \xi_k \end{bmatrix}\] <hr/> <h1 id="finite-horizon-problems">Finite Horizon Problems</h1> <p>Given an initial state \(x_0\) our goal is to find an optimal control input to the system \(\ref{eq:discrete_system}\), that minimizes the cost function</p> \[\underbrace{g_N(x_N)}_{\text{terminal cost}} + \underbrace{\sum_{k=0}^{N-1}\underbrace{g_k(x_k,u_k,w_k)}_\text{stage cost}}_\text{accumulated cost} \label{eq:cost_function}\] <p>while the horizon \(N\) is finite. Notice that \(W_0 = (w_0,...,w_{N-1})\) are random variables and thereby \(X_1 = (x_1,...,x_{N})\) are generally random variables as well, because they are correlated through the system dynamics. We therefore define the expected cost as</p> \[\text{E}_{(X_1,W_0|x_0)} \left[ g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k,u_k,w_k) \right] \label{eq:exp_cost}\] <h2 id="open-loop-vs-closed-loop-control">Open-loop vs. Closed-loop control</h2> <p><strong>Open-loop control</strong> means, there is no feedback and the entire control sequence is determined in the beginning. Given an initial state \(x_0\), we search a fixed sequence of control inputs \(U = (u_0,...,u_{N-1})\) that minimizes \(\ref{eq:exp_cost}\).</p> <p>In <strong>Closed-loop control</strong> at each timestep we know the current state. A policy \(\pi = (\mu_0(\cdot), ..., \mu_{N-1}(\cdot))\) maps each state at every timestep to a control input \(u_k = \mu_k(x_k)\in\mathcal{U}_k(x_k)\). Thus there is a feedback from the current state to the control input. Given an initial state \(x_0\), the expected cost associated with \(\pi\) becomes</p> \[J^\pi(x_0) = \text{E}_{(X_1,W_0|x_0)} \left[ g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k,\mu_k(x_k),w_k) \right] \label{eq:policy_exp_cost}\] <p>We look for an optimal policy \(\pi^*\) which minimizes the expected cost, yielding the optimal cost \(J^*(x) := J^{\pi^*}(x) \leq J^\pi(x), \forall \pi, \forall x\in\mathcal{S}_0.\)</p> <p>Without disturbance the system, and thus the cost, becomes deterministic. There is no benefit in a feedback, i.e. Open-loop and Closed-loop control are equivalent.</p> <h2 id="principle-of-optimality">Principle of Optimality</h2> <p>The Principle of Optimality makes it possible to split up the optimization problem into smaller subproblems. Casually speaking, when you have an optimal policy from time \(0\) to the end, then the policy is also optimal from an arbitrary time \(i\) to the end.</p> <blockquote class="notice--info"> <p>Let \(\pi^* = (\mu_0^*(\cdot), ..., \mu_i^*(\cdot), ..., \mu_{N-1}^*(\cdot))\) be an optimal policy. Consider the subproblem whereby we are at \(x_i \in S_i\) at time \(i\) with \(X_{i+1} = (x_{i+1},...,x_{N})\) and \(W_{i} = (x_i,...,x_{N})\). We want to minimize the cost from this time-step up until the end of the original problem</p> \[J_i(x_i) = \text{E}_{(X_{i+1},W_i|x_i)} \left[ g_N(x_N) + \sum_{k=i}^{N-1}g_k(x_k,\mu_k(x_k),w_k) \right]\] <p>Then the truncated policy \((\mu_i^*(\cdot), ..., \mu_{N-1}^*(\cdot))\) is optimal for the subproblem and for all possible \(x_i\). If it wasn’t, there would exist a policy yielding a lower cost on at least some portion of the state space. But this contradicts that \(\pi^*\) was an optimal policy in the first place.</p> </blockquote> <h2 id="the-dynamic-programming-algorithm">The Dynamic Programming Algorithm</h2> <p>Dynamic Programming uses the Principle of Optimality to recursively compute an optimal closed-loop policy and an optimal cost. Beginning from the terminal states at time N, going backwards towards time step 0.</p> <blockquote class="notice--info"> <p>Initialization</p> \[J_N(x) = g_N(x), \quad \forall x \in \mathcal{S}_N\] <p>Recursion \(k=(N-1),...,0\): For each \(x_k \in \mathcal{S}_k\) compute</p> \[J_k(x_k) = \min_{u\in\mathcal{U}_k(x)} \text{E}_{(w_k | x_k=x,u=u)} \left[ g_k(x_k,u,w_k) + J_{k+1}(f_k(x_k,u,w_k)) \right]\] <p>This gives us the optimal cost function \(J^*(x_0) = J_0(x_0)\). Furthermore the minimizing \(u^∗ =: µ^*_k (x_k)\) yields the optimal policy \(π^∗ = (µ^∗_0 (·) , µ^∗_1 (·) , . . . , µ^∗_{N −1} (·))\)</p> </blockquote> <h2 id="deterministic-finite-state-systems-and-the-shortest-path-problem">Deterministic Finite State Systems and the Shortest Path Problem</h2> <p>A Deterministic Finite State System is a special case of the discrete-state system: Namely the number of states is finite \(|S_k|&lt;\infty\). Furthermore the system is deterministic i.e. it has no disturbance \(w_k = 0\), which means we know exactly how our system will evolve given a certain input. As shown later, these DFS systems are equivalent to the Shortest Path Problem.</p> <hr/> <h1 id="infinite-horizon-problems">Infinite Horizon Problems</h1> <p>Bellman equation</p> <p>In the following we will exlusively deal with <a href="#discrete-state-systems">Finite-state Systems</a> and we want to minimize the expected cost as $N$ goes to infinity.</p> <h2 id="value-iteration">Value Iteration</h2> <h2 id="policy-iteration">Policy Iteration</h2> <h2 id="linear-programming">Linear Programming</h2> <hr/> <h1 id="deterministic-continuous-time-problems">Deterministic Continuous Time Problems</h1> <p>Bellmann Equation</p> \[123\] <h3 id="pontyagins-minimum-principle">Pontyagin’s Minimum Principle</h3> <p>Extra: How to derive the LQR from Pontyagins Principle</p>]]></content><author><name></name></author><category term="Uni Courses"/><category term="Optimal Control"/><category term="Dynamic Programming"/><category term="Bellman Equation"/><summary type="html"><![CDATA[Summary of Dynamic Programming and Optimal Control course at ETH Zürich]]></summary></entry></feed>